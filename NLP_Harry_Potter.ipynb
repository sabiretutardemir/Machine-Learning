{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO74Uljm7ahwHOd7JAAWo7T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sabiretutardemir/100-Day-ML-Codes-Challenge/blob/main/NLP_Harry_Potter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the harry_potter book. You can find this text corpus in the datasets\n",
        "folder.\n",
        "\n",
        "* Segment the text of the book into sentences. How many sentences does this book have?\n",
        "\n",
        "* Compute the frequency of each token in the book. What are the most frequent tokens?\n",
        "\n",
        "* Choose a sentence from the book. Analyze this chosen sentence by\n",
        "  * Calculating all n-grams.\n",
        "  * Finding POS tags of tokens.\n",
        "  * Stemming and lemmatizing tokens.\n",
        "\n",
        "* Check the documentation to identify the most important hyperparameters, attributes, and methods. Use them in practice."
      ],
      "metadata": {
        "id": "vjavvyAKsqJz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzMlNFdJsSwX",
        "outputId": "5858201d-cab9-4087-8068-4a205187826c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CHAPTER ONE THE BOY WHO LIVED \n",
            "\n",
            "Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal, thank you very much. They were the last people you'd expect to be involved in anything strange or mysterious, because they just didn't hold with such nonsense. \n",
            "\n",
            "Mr. Dursley was the director of a firm called Grunnings, which made drills. He was a big, beefy man with hardly any neck, although he did have a very large mustache. Mrs. Dursley was thin and blonde and \n"
          ]
        }
      ],
      "source": [
        "#load harry potter dataset\n",
        "\n",
        "with open('/content/harry_potter.txt', 'r', encoding='utf-8') as file:\n",
        "    book_text = file.read()\n",
        "\n",
        "# Now you can use book_text with NLTK or spaCy\n",
        "print(book_text[:500])  # Just printing the first 500 characters as a sample\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#How many sentences does this book have?\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "sentences = sent_tokenize(book_text)\n",
        "num_sentences = len(sentences)\n",
        "print(f\"The book has {num_sentences} sentences.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rz2CX6s2s88u",
        "outputId": "7dc226c1-377c-4d03-a2bc-a116c0f22639"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The book has 6394 sentences.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#do the same thing with spacy\n",
        "import spacy\n",
        "\n",
        "#Load the spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Create a spaCy document object\n",
        "doc = nlp(book_text)\n",
        "\n",
        "# Segment the text into sentences\n",
        "sentences = [sent.text for sent in doc.sents]\n",
        "\n",
        "# Print the number of sentences\n",
        "print(f'Total number of sentences: {len(sentences)}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yBS8pGXwIZa",
        "outputId": "313a8d42-15ac-4eef-f58b-1666aed75cd1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of sentences: 6186\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenize the text into words (tokens)\n",
        "tokens = word_tokenize(book_text.lower())  #lowercase for uniformity\n",
        "\n",
        "# Compute token frequency\n",
        "frequency = Counter(tokens)\n",
        "\n",
        "# Get the most common tokens\n",
        "most_common_tokens = frequency.most_common(10)  # Top 10 most frequent tokens\n",
        "\n",
        "print(\"Most frequent tokens:\", most_common_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvNCpKhpwknz",
        "outputId": "ca1d2e0f-5ca9-4715-814f-cc54ef4958a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent tokens: [(',', 5658), ('.', 5112), ('the', 3625), (\"''\", 2443), ('``', 2305), ('and', 1916), ('to', 1855), ('he', 1756), ('a', 1688), ('harry', 1324)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bEnYetuMwu9y",
        "outputId": "f5e238eb-ec96-4ae8-c0de-f331d2d5fe91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(',', 5658),\n",
              " ('.', 5112),\n",
              " ('the', 3625),\n",
              " (\"''\", 2443),\n",
              " ('``', 2305),\n",
              " ('and', 1916),\n",
              " ('to', 1855),\n",
              " ('he', 1756),\n",
              " ('a', 1688),\n",
              " ('harry', 1324)]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(book_text.lower())  #lowercase for uniformity\n",
        "\n",
        "# Extract tokens (filter out punctuation and spaces)\n",
        "tokens = [token.text for token in doc if not token.is_punct and not token.is_space]\n",
        "\n",
        "# Compute token frequency\n",
        "frequency = Counter(tokens)\n",
        "\n",
        "# Get the most common tokens\n",
        "most_common_tokens = frequency.most_common(10)  # Top 10 most frequent tokens\n",
        "\n",
        "print(\"Most frequent tokens:\", most_common_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBHiiunvwv0A",
        "outputId": "c4b347f1-76bf-434d-a065-c6b59ac4dfe4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most frequent tokens: [('the', 3627), ('and', 1919), ('to', 1861), ('he', 1756), ('a', 1690), ('harry', 1325), ('of', 1266), ('was', 1261), ('it', 1185), ('you', 1027)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "most_common_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wgKfNX-xNpq",
        "outputId": "320d0cd1-f692-402f-c519-b0f708b13eaa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('the', 3627),\n",
              " ('and', 1919),\n",
              " ('to', 1861),\n",
              " ('he', 1756),\n",
              " ('a', 1690),\n",
              " ('harry', 1325),\n",
              " ('of', 1266),\n",
              " ('was', 1261),\n",
              " ('it', 1185),\n",
              " ('you', 1027)]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "# 1. Tokenize the text into sentences\n",
        "sentences = sent_tokenize(book_text)\n",
        "\n",
        "# 2. Choose a random sentence\n",
        "random_sentence = random.choice(sentences)\n"
      ],
      "metadata": {
        "id": "WQGaqx4kxvnX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, ngrams, pos_tag\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Example sentence from the book (replace this with an actual sentence)\n",
        "sentence = random_sentence\n",
        "\n",
        "# 1. Tokenize the sentence\n",
        "tokens = word_tokenize(sentence.lower())  # Lowercased for consistency\n",
        "\n",
        "# 2. Calculate all n-grams (unigrams, bigrams, trigrams)\n",
        "unigrams = list(ngrams(tokens, 1))\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "trigrams = list(ngrams(tokens, 3))\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "# 3. Find POS tags\n",
        "pos_tags = pos_tag(tokens)\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# 4. Stemming (PorterStemmer)\n",
        "stemmer = PorterStemmer()\n",
        "stems = [stemmer.stem(token) for token in tokens]\n",
        "print(\"Stems:\", stems)\n",
        "\n",
        "# 5. Lemmatization (WordNetLemmatizer)\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "print(\"Lemmas:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIfyRpH3xPJj",
        "outputId": "73beff38-73f0-4394-8cec-bbb7affb6c86"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: [('hermione',), ('opened',), ('her',), ('mouth',), (',',), ('perhaps',), ('to',), ('tell',), ('ron',), ('exactly',), ('how',), ('to',), ('use',), ('the',), ('curse',), ('of',), ('the',), ('bogies',), (',',), ('but',), ('harry',), ('hissed',), ('at',), ('her',), ('to',), ('be',), ('quiet',), ('and',), ('beckoned',), ('them',), ('all',), ('forward',), ('.',)]\n",
            "Bigrams: [('hermione', 'opened'), ('opened', 'her'), ('her', 'mouth'), ('mouth', ','), (',', 'perhaps'), ('perhaps', 'to'), ('to', 'tell'), ('tell', 'ron'), ('ron', 'exactly'), ('exactly', 'how'), ('how', 'to'), ('to', 'use'), ('use', 'the'), ('the', 'curse'), ('curse', 'of'), ('of', 'the'), ('the', 'bogies'), ('bogies', ','), (',', 'but'), ('but', 'harry'), ('harry', 'hissed'), ('hissed', 'at'), ('at', 'her'), ('her', 'to'), ('to', 'be'), ('be', 'quiet'), ('quiet', 'and'), ('and', 'beckoned'), ('beckoned', 'them'), ('them', 'all'), ('all', 'forward'), ('forward', '.')]\n",
            "Trigrams: [('hermione', 'opened', 'her'), ('opened', 'her', 'mouth'), ('her', 'mouth', ','), ('mouth', ',', 'perhaps'), (',', 'perhaps', 'to'), ('perhaps', 'to', 'tell'), ('to', 'tell', 'ron'), ('tell', 'ron', 'exactly'), ('ron', 'exactly', 'how'), ('exactly', 'how', 'to'), ('how', 'to', 'use'), ('to', 'use', 'the'), ('use', 'the', 'curse'), ('the', 'curse', 'of'), ('curse', 'of', 'the'), ('of', 'the', 'bogies'), ('the', 'bogies', ','), ('bogies', ',', 'but'), (',', 'but', 'harry'), ('but', 'harry', 'hissed'), ('harry', 'hissed', 'at'), ('hissed', 'at', 'her'), ('at', 'her', 'to'), ('her', 'to', 'be'), ('to', 'be', 'quiet'), ('be', 'quiet', 'and'), ('quiet', 'and', 'beckoned'), ('and', 'beckoned', 'them'), ('beckoned', 'them', 'all'), ('them', 'all', 'forward'), ('all', 'forward', '.')]\n",
            "POS Tags: [('hermione', 'NN'), ('opened', 'VBD'), ('her', 'PRP$'), ('mouth', 'NN'), (',', ','), ('perhaps', 'RB'), ('to', 'TO'), ('tell', 'VB'), ('ron', 'RB'), ('exactly', 'RB'), ('how', 'WRB'), ('to', 'TO'), ('use', 'VB'), ('the', 'DT'), ('curse', 'NN'), ('of', 'IN'), ('the', 'DT'), ('bogies', 'NNS'), (',', ','), ('but', 'CC'), ('harry', 'VBP'), ('hissed', 'VBN'), ('at', 'IN'), ('her', 'PRP'), ('to', 'TO'), ('be', 'VB'), ('quiet', 'JJ'), ('and', 'CC'), ('beckoned', 'VBD'), ('them', 'PRP'), ('all', 'DT'), ('forward', 'RB'), ('.', '.')]\n",
            "Stems: ['hermion', 'open', 'her', 'mouth', ',', 'perhap', 'to', 'tell', 'ron', 'exactli', 'how', 'to', 'use', 'the', 'curs', 'of', 'the', 'bogi', ',', 'but', 'harri', 'hiss', 'at', 'her', 'to', 'be', 'quiet', 'and', 'beckon', 'them', 'all', 'forward', '.']\n",
            "Lemmas: ['hermione', 'opened', 'her', 'mouth', ',', 'perhaps', 'to', 'tell', 'ron', 'exactly', 'how', 'to', 'use', 'the', 'curse', 'of', 'the', 'bogy', ',', 'but', 'harry', 'hissed', 'at', 'her', 'to', 'be', 'quiet', 'and', 'beckoned', 'them', 'all', 'forward', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# 1. Process the sentence with spaCy\n",
        "doc = nlp(sentence.lower())\n",
        "\n",
        "# 2. Calculate n-grams manually (unigrams, bigrams, trigrams)\n",
        "unigrams = [token.text for token in doc]\n",
        "bigrams = [f'{doc[i].text} {doc[i+1].text}' for i in range(len(doc)-1)]\n",
        "trigrams = [f'{doc[i].text} {doc[i+1].text} {doc[i+2].text}' for i in range(len(doc)-2)]\n",
        "\n",
        "print(\"Unigrams:\", unigrams)\n",
        "print(\"Bigrams:\", bigrams)\n",
        "print(\"Trigrams:\", trigrams)\n",
        "\n",
        "# 3. Find POS tags\n",
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "print(\"POS Tags:\", pos_tags)\n",
        "\n",
        "# 4. Stemming (spaCy doesnâ€™t have a stemmer, but we can use the lemma as a similar approach)\n",
        "# 5. Lemmatization (spaCy's built-in lemmatizer)\n",
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(\"Lemmas:\", lemmas)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2qYytd9ycTD",
        "outputId": "9f66ac7a-8323-4d26-9b07-6248a027133c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unigrams: ['hermione', 'opened', 'her', 'mouth', ',', 'perhaps', 'to', 'tell', 'ron', 'exactly', 'how', 'to', 'use', 'the', 'curse', 'of', 'the', 'bogies', ',', 'but', 'harry', 'hissed', 'at', 'her', 'to', 'be', 'quiet', 'and', 'beckoned', 'them', 'all', 'forward', '.']\n",
            "Bigrams: ['hermione opened', 'opened her', 'her mouth', 'mouth ,', ', perhaps', 'perhaps to', 'to tell', 'tell ron', 'ron exactly', 'exactly how', 'how to', 'to use', 'use the', 'the curse', 'curse of', 'of the', 'the bogies', 'bogies ,', ', but', 'but harry', 'harry hissed', 'hissed at', 'at her', 'her to', 'to be', 'be quiet', 'quiet and', 'and beckoned', 'beckoned them', 'them all', 'all forward', 'forward .']\n",
            "Trigrams: ['hermione opened her', 'opened her mouth', 'her mouth ,', 'mouth , perhaps', ', perhaps to', 'perhaps to tell', 'to tell ron', 'tell ron exactly', 'ron exactly how', 'exactly how to', 'how to use', 'to use the', 'use the curse', 'the curse of', 'curse of the', 'of the bogies', 'the bogies ,', 'bogies , but', ', but harry', 'but harry hissed', 'harry hissed at', 'hissed at her', 'at her to', 'her to be', 'to be quiet', 'be quiet and', 'quiet and beckoned', 'and beckoned them', 'beckoned them all', 'them all forward', 'all forward .']\n",
            "POS Tags: [('hermione', 'NOUN'), ('opened', 'VERB'), ('her', 'PRON'), ('mouth', 'NOUN'), (',', 'PUNCT'), ('perhaps', 'ADV'), ('to', 'PART'), ('tell', 'VERB'), ('ron', 'PRON'), ('exactly', 'ADV'), ('how', 'SCONJ'), ('to', 'PART'), ('use', 'VERB'), ('the', 'DET'), ('curse', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('bogies', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('harry', 'PROPN'), ('hissed', 'VERB'), ('at', 'ADP'), ('her', 'PRON'), ('to', 'PART'), ('be', 'AUX'), ('quiet', 'ADJ'), ('and', 'CCONJ'), ('beckoned', 'VERB'), ('them', 'PRON'), ('all', 'PRON'), ('forward', 'ADV'), ('.', 'PUNCT')]\n",
            "Lemmas: ['hermione', 'open', 'her', 'mouth', ',', 'perhaps', 'to', 'tell', 'ron', 'exactly', 'how', 'to', 'use', 'the', 'curse', 'of', 'the', 'bogie', ',', 'but', 'harry', 'hiss', 'at', 'she', 'to', 'be', 'quiet', 'and', 'beckon', 'they', 'all', 'forward', '.']\n"
          ]
        }
      ]
    }
  ]
}